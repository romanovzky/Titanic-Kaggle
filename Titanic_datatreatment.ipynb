{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Packages and Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T15:21:48.233194Z",
     "start_time": "2018-03-03T15:21:47.731054Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, quantile_transform , minmax_scale\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "train_original=pd.read_csv('train.csv')\n",
    "train_original.set_index('PassengerId',inplace=True)\n",
    "train_full= pd.read_csv('train.csv')\n",
    "train_full.set_index('PassengerId',inplace=True)\n",
    "\n",
    "quantitative = [f for f in train_full.columns if train_full.dtypes[f] != 'object']\n",
    "quantitative.remove('Survived')\n",
    "qualitative = [f for f in train_full.columns if train_full.dtypes[f] == 'object']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data 3 needs the full list of names and tickets, we will focus only on data 1 and 2 for now, so that we do not have to do further engineering to prevent mistmatching number of features between train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T15:21:48.367100Z",
     "start_time": "2018-03-03T15:21:48.235002Z"
    }
   },
   "outputs": [],
   "source": [
    "#filling in missing values\n",
    "def clean_age(cols):\n",
    "    Age = cols[0]\n",
    "    Pclass = cols[1]\n",
    "    if pd.isnull(Age):\n",
    "\n",
    "        if Pclass == 1:\n",
    "            return 38\n",
    "\n",
    "        elif Pclass == 2:\n",
    "            return 30\n",
    "\n",
    "        else:\n",
    "            return 25\n",
    "\n",
    "    else:\n",
    "        return Age\n",
    "\n",
    "def clean_fare(cols):\n",
    "    Fare = cols[0]\n",
    "    Pclass = cols[1]\n",
    "    \n",
    "    if pd.isnull(Fare):\n",
    "\n",
    "        if Pclass == 1:\n",
    "            return 84.15\n",
    "\n",
    "        elif Pclass == 2:\n",
    "            return 20.66\n",
    "\n",
    "        else:\n",
    "            return 13.68\n",
    "\n",
    "    else:\n",
    "        return Fare\n",
    "    \n",
    "def set_age(data_set):\n",
    "    data_set['Age']=data_set[['Age','Pclass']].apply(clean_age,axis=1)\n",
    "    \n",
    "def set_fare(data_set):\n",
    "    data_set['Fare']=data_set[['Fare','Pclass']].apply(clean_age,axis=1)\n",
    "    \n",
    "def add_deck(data_set):\n",
    "    data_set['Deck']=data_set['Cabin'].apply(lambda x: x[0]+'_deck' if not pd.isnull(x) else 'Missing_Deck')\n",
    "\n",
    "def set_embark(data_set):\n",
    "    data_set['Embarked']=data_set['Embarked'].apply(lambda x: x if not pd.isnull(x) else 'Missing_Embark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T15:21:48.755133Z",
     "start_time": "2018-03-03T15:21:48.369456Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "def get_fare_and_family(data_set):\n",
    "    #Check if fare was paid (crew members would not have paid)\n",
    "    paid_fare=data_set['Fare'].apply(lambda x: 1 if x>0 else 0)\n",
    "    #Check if has family on board\n",
    "    has_family=(data_set['SibSp']>0) | (data_set['Parch']>0)\n",
    "    has_family=has_family.apply(lambda x: 1 if x else 0)\n",
    "    #we count a one-person family a family, this avoids dividing by 0\n",
    "    family_size= has_family*(data_set['SibSp']+data_set['Parch'])+1\n",
    "    #compute fare per person\n",
    "    fare_per_fam_member=data_set['Fare']/family_size\n",
    "    fare_and_family= pd.DataFrame(dict(paid_fare=paid_fare,has_family=has_family,family_size=family_size,fare_per_fam_member=fare_per_fam_member),index=data_set.index)\n",
    "    return fare_and_family\n",
    "    \n",
    "def get_duplicated_surnames_tickets_cabins(data_set):\n",
    "    #Check and identifies duplicated surnames, tickets, and cabin\n",
    "    duplicated_surnames_tickets_cabins=data_set[['Name','Ticket','Cabin']]\n",
    "    #find all duplicated/non-unique\n",
    "    duplicated_surnames_tickets_cabins['DuplicatedName']=duplicated_surnames_tickets_cabins['Name'].apply(lambda name: name.split(',',1)[0].split()[0]).duplicated(keep=False)\n",
    "    duplicated_surnames_tickets_cabins['DuplicatedTicket']=duplicated_surnames_tickets_cabins['Ticket'].duplicated(keep=False)\n",
    "    duplicated_surnames_tickets_cabins['DuplicatedCabin']=duplicated_surnames_tickets_cabins['Cabin'].duplicated(keep=False)\n",
    "    #find all duplicated/non-unique\n",
    "    duplicated_surnames_tickets_cabins['DuplicatedName']=duplicated_surnames_tickets_cabins['Name'].apply(lambda name: name.split(',',1)[0].split()[0]).duplicated(keep=False)\n",
    "    duplicated_surnames_tickets_cabins['DuplicatedTicket']=duplicated_surnames_tickets_cabins['Ticket'].duplicated(keep=False)\n",
    "    duplicated_surnames_tickets_cabins['DuplicatedCabin']=duplicated_surnames_tickets_cabins['Cabin'].duplicated(keep=False)\n",
    "    #turn trues and falses to 1s and 0s\n",
    "    duplicated_surnames_tickets_cabins['DuplicatedName']=duplicated_surnames_tickets_cabins['DuplicatedName'].apply(lambda x: 1 if x else 0)\n",
    "    duplicated_surnames_tickets_cabins['DuplicatedTicket']=duplicated_surnames_tickets_cabins['DuplicatedTicket'].apply(lambda x: 1 if x else 0)\n",
    "    duplicated_surnames_tickets_cabins['DuplicatedCabin']=duplicated_surnames_tickets_cabins['DuplicatedCabin'].apply(lambda x: 1 if x else 0)\n",
    "    #keep only the duplicatd surnames to one-hot them later\n",
    "    duplicated_surnames_tickets_cabins['Surname']='Unique_Surname'\n",
    "    is_duplicated = (duplicated_surnames_tickets_cabins['DuplicatedName']==1)\n",
    "    names=train_original['Name']\n",
    "    surnames=names.apply(lambda name: name.split(',',1)[0].split()[0] if name.split(',',1)[0].split()[0]!='van' else name.split(',',1)[0].split()[1])\n",
    "    duplicated_surnames_tickets_cabins.loc[is_duplicated,'Surname'] = surnames\n",
    "    #keep only the duplicated tickets to one-hot them later\n",
    "    duplicated_surnames_tickets_cabins['Tickets']='Unique_Ticket'\n",
    "    is_duplicated = (duplicated_surnames_tickets_cabins['DuplicatedTicket']==1)\n",
    "    tickets=train_original['Ticket']\n",
    "    duplicated_surnames_tickets_cabins.loc[is_duplicated,'Tickets'] = tickets\n",
    "    #keep only the duplicated cabins to one-hot them later\n",
    "    duplicated_surnames_tickets_cabins['Cabin']='Unique_Cabin'\n",
    "    is_duplicated = (duplicated_surnames_tickets_cabins['DuplicatedCabin']==1)\n",
    "    cabins=train_original['Cabin']\n",
    "    duplicated_surnames_tickets_cabins.loc[is_duplicated,'Cabin'] = cabins\n",
    "    return duplicated_surnames_tickets_cabins\n",
    "\n",
    "def get_titles(data_set):\n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_df['Title'] = data_set.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    temp_df['Title'] = temp_df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    temp_df['Title'] = temp_df['Title'].replace('Mlle', 'Miss')\n",
    "    temp_df['Title'] = temp_df['Title'].replace('Ms', 'Miss')\n",
    "    temp_df['Title'] = temp_df['Title'].replace('Mme', 'Mrs')\n",
    "    titles = temp_df['Title']\n",
    "    return titles\n",
    "\n",
    "def get_dummies(data_set):\n",
    "    duplicated_surnames_tickets_cabins=get_duplicated_surnames_tickets_cabins(data_set)\n",
    "    titles = get_titles(data_set)\n",
    "    #Dummies for sex, emark, titles, and the new created features\n",
    "    sex_dummies = pd.get_dummies(data_set['Sex'],drop_first=True)\n",
    "    embark_dummies = pd.get_dummies(data_set['Embarked'].apply(lambda x: x if not pd.isnull(x) else 'Missing_Embark'),drop_first=False)#We have missing embarked info, dropping first will overlap these with non-missing\n",
    "    cabin_dummies = pd.get_dummies(data_set['Cabin'],drop_first=False)#We have missing cabin info, dropping first will overlap these with non-missing\n",
    "    deck_dummies = pd.get_dummies(data_set['Cabin'].apply(lambda x: x[0]+'_deck' if not pd.isnull(x) else 'Missing_Deck'),drop_first=False)#We are not dropping Deck as we will try to predict these\n",
    "    titles_dummies = pd.get_dummies(titles,drop_first=True)\n",
    "    surname_dummies=pd.get_dummies(duplicated_surnames_tickets_cabins['Surname'],drop_first=True)\n",
    "    tickets_dummies=pd.get_dummies(duplicated_surnames_tickets_cabins['Tickets'],drop_first=True)\n",
    "    return sex_dummies, embark_dummies, cabin_dummies, deck_dummies, titles_dummies, surname_dummies,tickets_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T15:21:49.309430Z",
     "start_time": "2018-03-03T15:21:48.757600Z"
    }
   },
   "outputs": [],
   "source": [
    "# get scaled and transformed features\n",
    "def logtransform(data):\n",
    "    return np.log10(data+1)#+1 avoids issues with vanishing entries by shifting the log function to the left by 1\n",
    "def get_scalled_features (data_set):\n",
    "    fare_and_family = get_fare_and_family(data_set)\n",
    "    working_df=pd.concat([data_set,fare_and_family],axis=1)\n",
    "    #standartise age\n",
    "    temp=quantile_transform(working_df['Age'].values.reshape(-1,1),output_distribution='normal',n_quantiles=25,copy=True)\n",
    "    agestandard=pd.DataFrame(temp,index=working_df.index,columns=['AgeStandard'],copy=True)\n",
    "    #standartise fare\n",
    "    temp=quantile_transform(working_df['Fare'].values.reshape(-1,1),output_distribution='normal',n_quantiles=25,copy=True)\n",
    "    farestandard=pd.DataFrame(temp,index=working_df.index,columns=['FareStandard'],copy=True)\n",
    "    #standartise fare per family size\n",
    "    temp=quantile_transform(working_df['fare_per_fam_member'].values.reshape(-1,1),output_distribution='normal',n_quantiles=25,copy=True)\n",
    "    fare_fam_size_standard=pd.DataFrame(temp,index=working_df.index,columns=['fare_per_fam_member_Standard'],copy=True)\n",
    "    sibsp_scalled=working_df['SibSp'].apply(logtransform)\n",
    "    parch_scalled=working_df['Parch'].apply(logtransform)\n",
    "    famsize_scalled=working_df['family_size'].apply(np.log)\n",
    "    scalled_features = pd.concat([agestandard,farestandard,fare_fam_size_standard,sibsp_scalled,parch_scalled,famsize_scalled],axis=1)\n",
    "    return scalled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T15:21:51.519931Z",
     "start_time": "2018-03-03T15:21:51.506496Z"
    }
   },
   "outputs": [],
   "source": [
    "train_original = pd.read_csv('train.csv')\n",
    "train_original.set_index('PassengerId',inplace=True)\n",
    "test_original = pd.read_csv('test.csv')\n",
    "test_original.set_index('PassengerId',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T15:35:47.915286Z",
     "start_time": "2018-03-03T15:35:47.891660Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_data(data_set,data_type):\n",
    "    working_df=data_set\n",
    "    set_age(data_set)\n",
    "    set_fare(data_set)\n",
    "    add_deck(data_set)\n",
    "    set_embark(data_set)\n",
    "    scalled_features = get_scalled_features(data_set)\n",
    "    fare_and_family=get_fare_and_family(working_df)\n",
    "    duplicated_surnames_tickets_cabins=get_duplicated_surnames_tickets_cabins(working_df)\n",
    "    sex_dummies, embark_dummies, _, deck_dummies, titles_dummies, _,_ = get_dummies(working_df)\n",
    "    if(data_type=='1'):\n",
    "        data_transformed=pd.concat([data_set.drop(['Age','Fare','SibSp','Parch','Name','Sex','Ticket','Cabin','Embarked','Deck'],axis=1),scalled_features.drop(['fare_per_fam_member_Standard','family_size'],axis=1),sex_dummies,embark_dummies,deck_dummies],axis=1)\n",
    "    if(data_type=='2'):\n",
    "        data_transformed=pd.concat([working_df.drop(['Age','Fare','SibSp','Parch','Name','Sex','Ticket','Cabin','Embarked','Deck'],axis=1),scalled_features,fare_and_family['paid_fare'],sex_dummies,embark_dummies,deck_dummies,duplicated_surnames_tickets_cabins.drop([\"Name\",\"Ticket\",\"Cabin\",\"Surname\",\"Tickets\"],axis=1),titles_dummies],axis=1)\n",
    "    return data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T15:36:18.238595Z",
     "start_time": "2018-03-03T15:36:15.294247Z"
    }
   },
   "outputs": [],
   "source": [
    "data_to_prepare = ['1','2']\n",
    "\n",
    "for types in data_to_prepare:\n",
    "    train_working= train_original.copy()\n",
    "    test_working = test_original.copy()\n",
    "    test_working['Survived']=2\n",
    "    full_working = pd.concat([train_working,test_working])\n",
    "    full_transformed=transform_data(full_working,types)\n",
    "    train_transformed=full_transformed[full_transformed['Survived']<2]\n",
    "    test_transformed=full_transformed[full_transformed['Survived']==2].drop('Survived',axis=1)\n",
    "    full_transformed.to_csv(\"./data_raw_\"+types+\".csv\")\n",
    "    train_transformed.to_csv(\"./data_train_\"+types+\".csv\")\n",
    "    test_transformed.to_csv(\"./data_test_\"+types+\".csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data of type 1 (no new features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T12:35:49.118321Z",
     "start_time": "2018-02-21T12:35:48.335982Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_working= train_original.copy()\n",
    "test_working = test_original.copy()\n",
    "test_working['Survived']=2\n",
    "\n",
    "full_working = pd.concat([train_working,test_working])\n",
    "\n",
    "full_working_1=get_data_type_1(full_working)\n",
    "\n",
    "train_1=full_working_1[full_working_1['Survived']<2]\n",
    "test_1=full_working_1[full_working_1['Survived']==2].drop('Survived',axis=1)\n",
    "\n",
    "full_working_1.to_csv(\"./data_raw_1.csv\")\n",
    "train_1.to_csv(\"./data_train_1.csv\")\n",
    "test_1.to_csv(\"./data_test_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data of type 3 (new features and titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T12:35:57.359713Z",
     "start_time": "2018-02-21T12:35:56.021198Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_working= train_original.copy()\n",
    "test_working = test_original.copy()\n",
    "test_working['Survived']=2\n",
    "\n",
    "full_working = pd.concat([train_working,test_working])\n",
    "\n",
    "full_working_3=get_data_type_3(full_working)\n",
    "\n",
    "train_3=full_working_3[full_working_3['Survived']<2]\n",
    "test_3=full_working_3[full_working_3['Survived']==2].drop('Survived',axis=1)\n",
    "\n",
    "full_working_3.to_csv(\"./data_raw_3.csv\")\n",
    "train_3.to_csv(\"./data_train_3.csv\")\n",
    "test_3.to_csv(\"./data_test_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {
    "height": "317px",
    "width": "342px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
